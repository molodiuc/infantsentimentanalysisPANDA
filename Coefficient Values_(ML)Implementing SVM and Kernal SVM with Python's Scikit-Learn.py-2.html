
# coding: utf-8

# In[93]:


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
get_ipython().run_line_magic('matplotlib', 'inline')

bankdata = pd.read_csv("/Users/sofielysenko/Desktop/final_concavity_sixfeature_copy1.csv") # Reading the data in with pandas


# In[94]:


bankdata.shape # rows, columns


# In[75]:


bankdata.head() # see what data actually looks like


# In[5]:


#Data Preprocessing
#(1) Divide the data into attributes and labels
#(2) Divide the data into training and testing sets


# In[95]:


X = bankdata.drop('Emotion', axis=1) # removing the Class column bc that is labels
y = bankdata['Emotion'] # only Class column


# In[78]:


print(y)


# In[96]:


from sklearn.model_selection import train_test_split  # split into testing and training sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)  


# In[97]:


from sklearn import cross_validation
X_train, X_test, y_train, y_test = cross_validation.train_test_split(X,
                                                    y, test_size=0.33)


# In[98]:


from sklearn.model_selection import RepeatedKFold 

kf = RepeatedKFold(n_splits=146, n_repeats=50, random_state=None) 

for train_index, test_index in kf.split(X):
      print("Train:", train_index, "Validation:",test_index)
      X_train, X_test = X[train_index], X[test_index] 
      y_train, y_test = y[train_index], y[test_index]


# In[101]:


#Training the Algo

from sklearn.svm import SVC  
svclassifier = SVC(kernel='linear')
svclassifier.fit(X_train, y_train)  


# In[108]:


# Making Predictions
y_pred = svclassifier.predict(X_test)  


# In[109]:


#Evaluating the Algo
#Confusion matrix, precision, recall, and F1 measures are the most commonly used metrics for classification tasks
from sklearn.metrics import classification_report, confusion_matrix  
print(confusion_matrix(y_test,y_pred))  
print(classification_report(y_test,y_pred))  


# In[105]:


#Implementing Kernel SVM with Scikit-Learn


# In[61]:


import numpy as np  
import matplotlib.pyplot as plt  
import pandas as pd  


# In[63]:


#Importing the Dataset
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"

# Assign colum names to the dataset
colnames = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'Class']

# Read dataset to pandas dataframe
irisdata = pd.read_csv(url, names=colnames)  


# In[64]:


#Preprocessing
X = irisdata.drop('Class', axis=1)  
y = irisdata['Class']  


# In[65]:


#Train Test Split
from sklearn.model_selection import train_test_split  
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)  


# In[66]:


#1. Polynomial Kernel

from sklearn.svm import SVC  
svclassifier = SVC(kernel='poly', degree=8)  
svclassifier.fit(X_train, y_train)  

#Making predictions
y_pred = svclassifier.predict(X_test)  

#Evaluate 
from sklearn.metrics import classification_report, confusion_matrix  
print(confusion_matrix(y_test, y_pred))  
print(classification_report(y_test, y_pred))  


# In[67]:


#2. Gaussian Kernel
#(different setup)
from sklearn.svm import SVC  
svclassifier = SVC(kernel='rbf')  
svclassifier.fit(X_train, y_train)  

#Prediction (same)
y_pred = svclassifier.predict(X_test)  

#Evaluate (same)
from sklearn.metrics import classification_report, confusion_matrix  
print(confusion_matrix(y_test, y_pred))  
print(classification_report(y_test, y_pred))  


# In[69]:


#3. Sigmoid Kernel
from sklearn.svm import SVC  
svclassifier = SVC(kernel='sigmoid')  
svclassifier.fit(X_train, y_train)  

y_pred = svclassifier.predict(X_test)  

from sklearn.metrics import classification_report, confusion_matrix  
print(confusion_matrix(y_test, y_pred))  
print(classification_report(y_test, y_pred))  


# ## If we compare the performance of the different types of kernels we can clearly see that the sigmoid kernel performs the worst. This is due to the reason that sigmoid function returns two values, 0 and 1, therefore it is more suitable for binary classification problems. However, in our case we had three output classes

# # Amongst the Gaussian kernel and polynomial kernel, we can see that Gaussian kernel achieved a perfect 100% prediction rate while polynomial kernel misclassified one instance. Therefore the Gaussian kernel performed slightly better. However, there is no hard and fast rule as to which kernel performs best in every scenario. It is all about testing all the kernels and selecting the one with the best results on your test dataset.

# In[111]:


import matplotlib.pyplot as plt

plt.plot([1,2,3,4], [1,2,3,4])
plt.ylabel('some numbers')
plt.show()


# In[133]:


import matplotlib.pyplot as plt  #plotting cross validation and emotion paramaters



#mean curvature of eyes_top v. curvature of mouth_top

plt.plot([-0.00806411,
-0.01589476,
-0.01160663,
-0.01025176,
-0.00311819,
-0.00311819,
-0.01160663,
-0.01025176,
-0.00311819,
-0.01342401,
-0.00908403,
-0.01770433,
-0.00675553,
-0.00494765,
-0.00463582,
-0.0084853,
-0.01111215,
-0.01383382,
-0.01378833,
-0.00742621,
-0.01065938,
-0.01170681,
-0.01170681,
-0.00968592,
-0.00479239,
-0.01053609,
-0.01676623,
-0.00649448,
-0.04080943,
-0.01543992,
-0.00734975,
-0.0101308,
-0.01101356,
-0.01205509,
-0.01656136,
-0.01324697,
-0.00848422,
-0.01089667,
-0.0067849,
-0.01067126,
-0.02367693,
-0.00886555,
-0.0215474,
-0.02224071,
-0.02224071,
-0.0042364,
-0.00368749,
-0.00368749,
-0.01282005,
-0.01327951,
-0.0110624,
-0.0117359,
-0.00724189,
-0.01134828,
-0.01935585,
-0.00958579,
-0.00985592,
-0.01638157,
-0.00764207,
-0.00764207,
-0.06870794], [-0.006518755,
-0.008877615,
-0.00761043,
-0.00824203,
-0.00674816,
-0.00674816,
-0.00761043,
-0.00824203,
-0.00674816,
-0.001993585,
-0.00858066,
-0.009377765,
-0.00280986,
-0.009249925,
-0.00035805,
-0.001266414772,
-0.014741315,
-0.011204745,
-0.006600885,
-0.00419438,
-0.006411365,
-0.00878944,
-0.00878944,
-0.00773097,
-0.010713985,
-0.003269275,
-0.008438265,
-0.00900733,
-0.016694445,
-0.0080187,
-0.005314275,
-0.00925217,
-0.004141405,
-0.01187232,
-0.007665355,
-0.00942107,
-0.004023245,
-0.005430415,
-0.00682665,
-0.00827142,
-0.009423155,
-0.005675285,
-0.011282435,
-0.01351077,
-0.01351077,
-0.00843225,
-0.00746669,
-0.00746669,
-0.00486183,
-0.01017229,
-0.00377968,
-0.004918515,
-0.007296505,
-0.00551844,
-0.006274305,
-0.00197358,
-0.01148569,
-0.004669195,
-0.01005287,
-0.01005287,
-0.06666667,

], "yo", label = "Positive") #Positive


plt.plot([-0.00632528,
-0.01054894,
-0.01054894,
-0.01054894,
-0.01054894,
-0.01210659,
-0.01210659,
-0.00920739,
-0.00390145,
-0.01050513,
-0.0103832,
-0.01209897,
-0.01209897,
-0.0161115,
-0.0161115,
-0.00764177,
-0.01514055,
-0.00950656,
-0.01000238,
-0.01000238,
-0.00737922,
-0.00673623,
-0.00673623,
-0.01477629,
-0.00087657,
-0.00087657,
-0.01451691,
-0.01451691,
-0.00868048,
-0.00196974,
-0.00726705,
-0.02149493,
-0.00569533,
-0.00394741,
-0.03413358,
-0.01490169,
-0.00659931,
-0.00761598,
-0.01379964,
-0.00224749,
-0.00266443,
-0.01733787,
-0.00911591,
-0.0073498,
-0.01425589,
-0.01536487,
-0.01113472,
-0.00589383,
-0.00968274], [-0.007562615,
-0.00668079,
-0.00668079,
-0.00668079,
-0.00668079,
-0.00258631,
-0.00258631,
0.001263959811,
-0.003569145,
-0.002406855,
-0.00257888,
-0.00388262,
-0.00388262,
-0.00905484,
-0.00905484,
-0.004097115,
-0.00300311,
-0.009022355,
-0.00332557,
-0.00332557,
-0.006944945,
-0.00637214,
-0.00637214,
-0.011252155,
-0.00647785,
-0.00647785,
-0.00342047,
-0.00342047,
-0.005677565,
-0.004601335,
-0.00258945,
-0.01083616,
-0.0145334,
-0.002535885,
-0.03618197,
-0.002529885,
-0.00531771,
-0.0029355,
-0.00365497,
-0.01050821,
-0.008622125,
-0.00654587,
-0.001391485,
3.15E-04,
0.0026387,
-0.00437962,
0.00036859,
-0.008451715,
-0.000477675],"bo", label= 'Negative') #Negative


plt.plot([-0.0090756,
-0.00735513,
-0.01974855,
-0.01347077,
-0.00998419,
-0.00389741,
-0.0195448,
-0.05693324,
-0.0066998,
-0.0126551,
-0.00720337,
-0.00720337,
-0.00658918,
-0.00924476,
-0.01398233,
-0.00524069,
-0.01619646,
-0.02141228,
-0.01269043,
-0.00617308,
-0.0128979,
-0.01256407,
-0.02038972,
-0.00615366,
-0.01318539,
-0.01689665,
-0.00289739,
-0.01394908,
-0.00711871,
-0.00727892,
-0.00251227,
-0.02010163,
-0.00775365,
-0.00735879,
-0.00699078,
-0.00703828], [-0.01346618,
-0.00909097,
-0.04674755,
-0.010857525,
-0.01695174,
-0.010727625,
-0.015819455,
-0.0459596,
-0.011195145,
-0.015553535,
-0.0098414,
-0.0098414,
-0.008742785,
-0.0182225,
-0.01125608,
-0.011157715,
-0.0131913,
-0.02028265,
-0.0117589,
-0.01003293,
-0.015322425,
-0.011171765,
-0.01797139,
-0.01074623,
-0.01311073,
-0.02705113,
-0.014152505,
-0.013876535,
-0.014175015,
-0.011316025,
-0.020318135,
-0.014774325,
-0.008984615,
-0.01370141,
-0.005908525,
-0.01240508],"ro", label= 'Neutral') #Neutral
 #Neutral
    
plt.ylabel('mean curvature of eyes_top')
plt.xlabel('curvature of mouth_top')
plt.show()

